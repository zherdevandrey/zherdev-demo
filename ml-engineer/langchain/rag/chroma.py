from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import CharacterTextSplitter

if __name__ == '__main__':
    # –ó–∞–≥—Ä—É–∑–∫–∞ PDF
    pdf = PyPDFLoader("Introduction_to_Data_and_Data_Science.pdf")
    pages = pdf.load()

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
    splitter = CharacterTextSplitter(separator=".", chunk_size=500, chunk_overlap=50)  # –î–æ–±–∞–≤–ª–µ–Ω overlap

    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    for i, page in enumerate(pages):
        page.page_content = page.page_content.replace('\n', ' ')

    pages_split = splitter.split_documents(pages)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    embeddings = OllamaEmbeddings(model="nomic-embed-text")

    # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å)
    vector_store = Chroma.from_documents(
        documents=pages_split,
        embedding=embeddings,  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: embedding –≤–º–µ—Å—Ç–æ embeddings
        persist_directory="./intro-to-ds-lecture"
    )

    print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –±–∞–∑–∞ —Å–æ–∑–¥–∞–Ω–∞ —Å {len(pages_split)} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    vector_store_loaded = Chroma(
        persist_directory="./intro-to-ds-lecture",
        embedding_function=embeddings  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: embedding_function –≤–º–µ—Å—Ç–æ embeddings
    )

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞
    query = "—á—Ç–æ —Ç–∞–∫–æ–µ data science"
    results = vector_store_loaded.similarity_search(query, k=3)

    print(f"\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}':")
    for i, doc in enumerate(results):
        print(f"{i + 1}. {doc.page_content[:100]}...")